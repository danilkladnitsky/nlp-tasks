{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework #1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cookie Manager: \"Don't allow sites that set removed cookies to set future cookies\" should stay checked\n",
      "When in full screen mode\n",
      "Pressing Ctrl-N should open a new browser when only download dialog is\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package webtext to\n",
      "[nltk_data]     /Users/partnadem/nltk_data...\n",
      "[nltk_data]   Package webtext is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "import nltk.corpus\n",
    "from nltk.corpus import webtext\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "\n",
    "     nltk.download('webtext')\n",
    "        plain_text = webtext.raw()\n",
    "\n",
    "      print( plain_text[:200]   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing plain text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Cookie', 'Manager', ':', '``', 'Do', \"n't\", 'allow', 'sites', 'that', 'set'] 366313\n"
     ]
    }
   ],
   "source": [
    "raw_token_list = word_tokenize(plain_text, 'en', True)\n",
    "print(raw_token_list[:10], len(raw_token_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deleting stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/partnadem/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Cookie', 'Manager', ':', '``', 'Do', \"n't\", 'allow', 'sites', 'set', 'removed']\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "\n",
    "forbidden_words = stopwords.words('english')\n",
    "\n",
    "cleared_from_stopwords_tokens = list()\n",
    "\n",
    "for token in raw_token_list:\n",
    "    if token not in forbidden_words:\n",
    "        cleared_from_stopwords_tokens.append(token)\n",
    "\n",
    "print(cleared_from_stopwords_tokens[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cookie', 'manager', 'do', 'allow', 'sites', 'set', 'removed', 'cookies', 'set', 'future', 'cookies', 'stay', 'checked', 'when', 'full', 'screen', 'mode', 'pressing', 'open', 'new', 'browser', 'download', 'dialog', 'left', 'open', 'add', 'icons', 'context', 'menu', 'so']\n"
     ]
    }
   ],
   "source": [
    "special_character_regex = '[^a-zA-Z0-9]+'\n",
    "\n",
    "normalized_token_list = list()\n",
    "\n",
    "for token in cleared_from_stopwords_tokens:\n",
    "    is_forbidden_word = bool(re.search(special_character_regex, token))\n",
    "\n",
    "    if is_forbidden_word:\n",
    "        continue\n",
    "\n",
    "    normalized_token_list.append(token.lower())\n",
    "\n",
    "print(normalized_token_list[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking frequency of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'i': 7803, 'girl': 2938, 'guy': 2725, '1': 2091, 'like': 1654, '2': 1647, 'you': 1303, 'man': 984, 'woman': 979, 'know': 906, ...})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequency_map = FreqDist()\n",
    "\n",
    "for token in normalized_token_list:\n",
    "    frequency_map[token.lower()]+=1\n",
    "\n",
    "frequency_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized ['cookie', 'manager', 'do', 'allow', 'sites', 'set', 'removed', 'cookies', 'set', 'future']\n",
      "stemmed ['cooki', 'manag', 'do', 'allow', 'site', 'set', 'remov', 'cooki', 'set', 'futur']\n"
     ]
    }
   ],
   "source": [
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "stemmed_tokens = list()\n",
    "\n",
    "for token in normalized_token_list:\n",
    "    stemmed_tokens.append(porter_stemmer.stem(token))\n",
    "\n",
    "print('normalized', normalized_token_list[:10])\n",
    "print(\"stemmed\", stemmed_tokens[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/partnadem/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized ['cookie', 'manager', 'do', 'allow', 'sites', 'set', 'removed', 'cookies', 'set', 'future']\n",
      "lemmatized ['cookie', 'manager', 'do', 'allow', 'site', 'set', 'removed', 'cooky', 'set', 'future']\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"wordnet\")\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "lemmatized_tokens = list()\n",
    "\n",
    "for token in normalized_token_list:\n",
    "    lemmatized_tokens.append(lemmatizer.lemmatize(token))\n",
    "\n",
    "print(\"normalized\", normalized_token_list[:10])\n",
    "print(\"lemmatized\", lemmatized_tokens[:10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
